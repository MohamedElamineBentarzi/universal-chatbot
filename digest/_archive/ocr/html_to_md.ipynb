{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b623a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup, Comment\n",
    "from IPython.display import display, HTML , Markdown\n",
    "from pathlib import Path\n",
    "import json\n",
    "from markdownify import markdownify as md\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import requests\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e329c01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA_DIR = \"data\\\\resources_raw\"\n",
    "HTML_FILES = [Path(RAW_DATA_DIR) / html_name for html_name in os.listdir(RAW_DATA_DIR) if html_name.endswith(\".html\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac6aa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def collect_hash_sources(root_dir):\n",
    "    \"\"\"\n",
    "    Recursively traverse a directory structure, read all JSON files,\n",
    "    and build a dictionary mapping:\n",
    "\n",
    "      html_hash      -> url\n",
    "      external_hash  -> external_url\n",
    "      file_hash      -> file_download_url\n",
    "\n",
    "    pdf_hash is IGNORED.\n",
    "    \"\"\"\n",
    "    hash_map = {}\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if not filename.lower().endswith(\".json\"):\n",
    "                continue\n",
    "\n",
    "            json_path = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Parse JSON\n",
    "            try:\n",
    "                with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    data = json.load(f)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Could not parse {json_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # ---- html_hash → url ----\n",
    "            if \"html_hash\" in data and data[\"html_hash\"]:\n",
    "                if \"url\" in data and data[\"url\"]:\n",
    "                    hash_map[data[\"html_hash\"]] = data[\"url\"]\n",
    "\n",
    "            # ---- external_hash → external_url ----\n",
    "            if \"external_hash\" in data and data[\"external_hash\"]:\n",
    "                if \"external_url\" in data and data[\"external_url\"]:\n",
    "                    hash_map[data[\"external_hash\"]] = data[\"external_url\"]\n",
    "\n",
    "            # ---- file_hash → file_download_url ----\n",
    "            if \"file_hash\" in data and data[\"file_hash\"]:\n",
    "                if \"file_download_url\" in data and data[\"file_download_url\"]:\n",
    "                    hash_map[data[\"file_hash\"]] = data[\"file_download_url\"]\n",
    "\n",
    "            # pdf_hash IS IGNORED by your request\n",
    "\n",
    "    return hash_map\n",
    "HASH_TO_URL_MAP = collect_hash_sources(\"D:\\\\Projects\\\\Python\\\\btp-rag\\\\data\\\\resources_metadata\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e73594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean HTML by removing scripts, styles, comments,\n",
    "    inline CSS, noisy attributes, and empty tags.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # --- Remove scripts & styles ---\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # --- Remove comments ---\n",
    "    for c in soup.find_all(string=lambda text: isinstance(text, Comment)):\n",
    "        c.extract()\n",
    "\n",
    "    # --- Remove inline styles & noisy attributes ---\n",
    "    noisy_attrs = [\"class\", \"id\", \"style\", \"onclick\", \"onload\", \"width\", \"height\"]\n",
    "    for tag in soup.find_all():\n",
    "        for a in noisy_attrs:\n",
    "            if a in tag.attrs:\n",
    "                del tag[a]\n",
    "\n",
    "    # --- Remove <head> entirely (keeps only content skeleton) ---\n",
    "    if soup.head:\n",
    "        soup.head.decompose()\n",
    "\n",
    "    # --- Remove empty tags except img ---\n",
    "    for tag in soup.find_all():\n",
    "        if tag.name == \"img\":\n",
    "            continue  # KEEP IMAGES\n",
    "        if not tag.get_text(strip=True) and not tag.contents:\n",
    "            tag.decompose()\n",
    "\n",
    "\n",
    "    return str(soup)\n",
    "\n",
    "\n",
    "def html_to_markdown(html: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert cleaned HTML to Markdown.\n",
    "    \"\"\"\n",
    "    clean = clean_html(html)\n",
    "    markdown = md(clean, heading_style=\"ATX\")\n",
    "    return markdown.strip()\n",
    "\n",
    "\n",
    "def convert_directory_to_markdown(directory: str):\n",
    "    \"\"\"\n",
    "    Process all .html files in a directory, convert\n",
    "    each to clean Markdown, and display in notebook.\n",
    "    \"\"\"\n",
    "    html_files = [f for f in os.listdir(directory) if f.lower().endswith(\".html\")]\n",
    "\n",
    "    if not html_files:\n",
    "        print(\"No HTML files found.\")\n",
    "        return\n",
    "\n",
    "    for file in html_files:\n",
    "        path = os.path.join(directory, file)\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            raw = f.read()\n",
    "\n",
    "        md_content = html_to_markdown(raw)\n",
    "\n",
    "        display(Markdown(f\"### **{file}**\\n\\n```\\n{md_content}\\n```\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9faf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1177 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1177/1177 [00:20<00:00, 56.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 1177 HTML files, 142 have local image references.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def has_local_image_refs(html: str) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the HTML contains <img> tags whose src is NOT\n",
    "    an absolute URL (http/https) but a local relative reference.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        src = img.get(\"src\")\n",
    "\n",
    "        # Missing or empty src → skip\n",
    "        if not src:\n",
    "            continue\n",
    "        \n",
    "        src = src.strip()\n",
    "\n",
    "        # Ignore fully qualified URLs\n",
    "        if src.startswith(\"http://\") or src.startswith(\"https://\"):\n",
    "            continue\n",
    "        \n",
    "        # Everything else = local file reference\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "count_with_local_imgs = 0\n",
    "\n",
    "for file in tqdm(HTML_FILES):\n",
    "    with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw = f.read()\n",
    "\n",
    "    if has_local_image_refs(raw):\n",
    "        count_with_local_imgs += 1\n",
    "        \n",
    "print(f\"Out of {len(HTML_FILES)} HTML files, {count_with_local_imgs} have local image references.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c612809d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_markdown_and_images(html_path: str, output_base_dir: str, HASH_TO_URL_MAP: dict):\n",
    "    \"\"\"\n",
    "    Convert HTML to Markdown, download images, and save everything\n",
    "    in a directory named after the input HTML file.\n",
    "    This version correctly resolves relative images using the original source URL.\n",
    "    \"\"\"\n",
    "\n",
    "    # -----------------------------\n",
    "    # Load raw HTML\n",
    "    # -----------------------------\n",
    "    with open(html_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        raw_html = f.read()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract hash from filename\n",
    "    # -----------------------------\n",
    "    filename = os.path.splitext(os.path.basename(html_path))[0]\n",
    "    html_hash = filename  # name *is* the hash\n",
    "\n",
    "    # -----------------------------\n",
    "    # Determine original page URL\n",
    "    # -----------------------------\n",
    "    original_url = HASH_TO_URL_MAP.get(html_hash)\n",
    "    if not original_url:\n",
    "        raise ValueError(f\"❌ No source URL found for hash: {html_hash}\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Build output directory\n",
    "    # -----------------------------\n",
    "    out_dir = os.path.join(output_base_dir, filename)\n",
    "    img_dir = os.path.join(out_dir, \"images\")\n",
    "    os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Clean HTML but KEEP IMAGES\n",
    "    # -----------------------------\n",
    "    soup = BeautifulSoup(raw_html, \"html.parser\")\n",
    "\n",
    "    # Remove scripts & styles\n",
    "    for tag in soup([\"script\", \"style\"]):\n",
    "        tag.decompose()\n",
    "\n",
    "    # Remove comments\n",
    "    for c in soup.find_all(string=lambda t: isinstance(t, Comment)):\n",
    "        c.extract()\n",
    "\n",
    "    # Remove noisy attributes\n",
    "    noisy_attrs = [\"class\", \"id\", \"style\", \"onclick\", \"onload\", \"width\", \"height\"]\n",
    "    for tag in soup.find_all():\n",
    "        for a in noisy_attrs:\n",
    "            if a in tag.attrs:\n",
    "                del tag[a]\n",
    "\n",
    "    # Remove <head>\n",
    "    if soup.head:\n",
    "        soup.head.decompose()\n",
    "\n",
    "    # Remove empty tags EXCEPT <img>\n",
    "    for tag in soup.find_all():\n",
    "        if tag.name == \"img\":\n",
    "            continue\n",
    "        if not tag.get_text(strip=True) and not tag.contents:\n",
    "            tag.decompose()\n",
    "\n",
    "    cleaned_html = str(soup)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Convert to Markdown\n",
    "    # -----------------------------\n",
    "    markdown = md(cleaned_html, heading_style=\"ATX\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract and download images\n",
    "    # -----------------------------\n",
    "    images = soup.find_all(\"img\")\n",
    "    img_map = {}  # old_url → new_local_path\n",
    "\n",
    "    for idx, img in enumerate(images, start=1):\n",
    "        src = img.get(\"src\")\n",
    "        if not src or not src.strip():\n",
    "            continue\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # Resolve final image URL via base page URL\n",
    "        # ------------------------------------------\n",
    "        # If src is absolute → keep as is\n",
    "        if src.startswith(\"http://\") or src.startswith(\"https://\"):\n",
    "            abs_url = src\n",
    "        else:\n",
    "            # src is relative → resolve using original URL\n",
    "            abs_url = urljoin(original_url, src)\n",
    "\n",
    "        # ------------------------------------------\n",
    "        # Download the image\n",
    "        # ------------------------------------------\n",
    "        try:\n",
    "            r = requests.get(abs_url, timeout=10)\n",
    "            if r.status_code == 200:\n",
    "                # Compute extension\n",
    "                ext = os.path.splitext(urlparse(abs_url).path)[1] or \".jpg\"\n",
    "                local_name = f\"img_{idx:03d}{ext}\"\n",
    "                local_path = os.path.join(img_dir, local_name)\n",
    "\n",
    "                with open(local_path, \"wb\") as f:\n",
    "                    f.write(r.content)\n",
    "\n",
    "                img_map[src] = f\"images/{local_name}\"\n",
    "\n",
    "        except Exception:\n",
    "            # If download fails → do not include broken link\n",
    "            continue\n",
    "\n",
    "    # -----------------------------\n",
    "    # Rewrite image URLs in markdown\n",
    "    # -----------------------------\n",
    "    for original, local in img_map.items():\n",
    "        markdown = markdown.replace(original, local)\n",
    "\n",
    "    # -----------------------------\n",
    "    # Save final Markdown\n",
    "    # -----------------------------\n",
    "    md_path = os.path.join(out_dir, f\"{filename}.md\")\n",
    "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0e0f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1177/1177 [2:23:30<00:00,  7.32s/it] \n"
     ]
    }
   ],
   "source": [
    "output_dir = \"data/resources_md\" # Output directory for markdown files and images\n",
    "for html_file in tqdm(HTML_FILES):\n",
    "    save_markdown_and_images(html_file, output_dir, HASH_TO_URL_MAP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".btp-rag-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
